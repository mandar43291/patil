import bs4 as bs
from sys import *
from urllib.request import urlopen
import urllib
import csv

url = "https://www.amazon.in/Redmi-Note-Pro-Electric-Processor/product-reviews/B07X2KLYSF/ref=cm_cr_arp_d_paging_btm_next_2?ie=UTF8&reviewerType=all_reviews&pageNumber=2"
resultMid = url.index('next_')
resultLst = url.index('ber=')

indexMid = resultMid + 5
indexLst = resultLst + 4

indexLstTD = resultLst + 5

indexLstTh = resultLst + 6
comments = []
urlst = []

# concatanation of string take place in below function 
def replace_str_index(text,index=0,replacement=''):
    return '%s%s%s'%(text[:index],replacement,text[index+1:])
#add number of pages you want to fetch(range)
for i in range(1,101):
    if(i < 9):
        first = replace_str_index(url,indexMid,i)
        second = replace_str_index(first,indexLst,i)
        urlst.append(second)
    elif(i > 9):
        first = replace_str_index(url,indexMid,i)
        second = replace_str_index(first,indexLstTD,i)
        urlst.append(second)
    elif(i > 99):
        first = replace_str_index(url,indexMid,i)
        second = replace_str_index(first,indexLstTh,i)
        urlst.append(second)
    
for i in urlst:
    source = urllib.request.urlopen(i)
    soup = bs.BeautifulSoup(source, 'html.parser')
    container = soup.find_all("div", {"class": "a-row a-spacing-small review-data"})

    for txt in container:
        comments.append(txt.text)
        
# file created to store the given comments
fd =open("Fi101.csv", 'w' encoding="utf-8")
for rmDiv in comments:
    fd.write(rmDiv)
fd.close()
